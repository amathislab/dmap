{"env_name": "AdaptHopperBulletEnv", "env_config": {"sigma": 0.1, "render": false, "num_memory_steps": 30}, "num_trainer_workers": 9, "policies_to_train": {"policy": true}, "policy_classes": {"policy": "sac"}, "policy_configs": {"policy": {"policy_model": {"custom_model": "dmap_icl", "fcnet_activation": "relu", "feature_convnet_params": [{"num_filters": 32, "kernel_size": 5, "stride": 4}, {"num_filters": 32, "kernel_size": 3, "stride": 1}, {"num_filters": 32, "kernel_size": 3, "stride": 1}], "feature_fcnet_hiddens": [128, 32], "policy_hiddens": [32, 32], "embedding_size": 4, "custom_model_config": {"steps": 36}}, "Q_model": {"custom_model": "oracle_q_adapt", "fcnet_activation": "relu", "encoder_hiddens": [256, 128], "encoding_size": 4, "q_hiddens": [128, 128]}, "normalize_actions": true, "no_done_at_end": false, "n_step": 1, "timesteps_per_iteration": 1500, "prioritized_replay": true, "buffer_size": 300000, "optimization": {"actor_learning_rate": 0.0003, "critic_learning_rate": 0.0003, "entropy_learning_rate": 0.0003}, "grad_clip": null, "learning_starts": 10000, "target_network_update_freq": 0, "worker_side_prioritization": false, "min_iter_time_s": 1}}, "agent_policy_dict": {"agent": "policy"}, "train": true, "gamma": 0.995, "rollout_fragment_length": 1, "train_batch_size": 256, "lr": 0.0001, "num_gpus": 1, "extra_trainer_params": {"framework": "torch", "seed": 0}, "trainer_class": "sac", "episodes_total": 10000000.0, "logdir": "output", "trial_name": "dmap_hopper_sigma_01", "checkpoint_freq": 100, "restore_checkpoint_path": null, "rollout_num_episodes": 1000}