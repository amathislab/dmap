{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from definitions import ROOT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_word(extended_word, word_list):\n",
    "    for w in word_list:\n",
    "        if w in extended_word:\n",
    "            return w\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the datasets with the performance evaluations on fixed target environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_names = [\"ant\", \"hopper\", \"walker\", \"half_cheetah\"]\n",
    "algos = (\"attention_no_encoding\", \"simple\", \"rma_phase_1\", \"attention\", \"encoding\", \"convnet\", \"adapted\")\n",
    "algo_name_dict = {\n",
    "    \"attention_no_encoding\": \"dmap-ne\",\n",
    "    \"simple\": \"simple\",\n",
    "    \"rma_phase_1\": \"oracle\",\n",
    "    \"attention\": \"dmap\",\n",
    "    \"encoding\": \"oracle\",\n",
    "    \"convnet\": \"tcn\",\n",
    "    \"adapted\": \"rma\"\n",
    "}\n",
    "datasets = (\"sigma_01\", \"sigma_03\", \"sigma_05\", \"sigma_07\",\n",
    "            \"sigma_between_0_01\", \"sigma_between_0_03\", \"sigma_between_0_05\", \"sigma_between_0_07\")\n",
    "train_sigma_list = (\"sigma_01\", \"sigma_03\", \"sigma_05\")\n",
    "seed_list = (\"seed_0\", \"seed_1\", \"seed_2\", \"seed_3\", \"seed_4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_summary_dict = {}\n",
    "\n",
    "for name in env_names:\n",
    "    env_summary_dict[name] = []\n",
    "    result_dir = os.path.join(ROOT_DIR, \"data\", name, \"performance\")\n",
    "    result_dir_content = os.listdir(result_dir)\n",
    "    \n",
    "    result_json_list = []\n",
    "    for file_name in result_dir_content:\n",
    "        file_path = os.path.join(result_dir, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            assert \".json\" in file_name\n",
    "            with open(file_path, \"r\") as file:\n",
    "                result_json = json.load(file)\n",
    "            result_json_list.append(result_json)\n",
    "\n",
    "    for result_json in result_json_list:\n",
    "        for experiment_name, data in result_json.items():\n",
    "            algorithm_name = find_word(experiment_name, algos)\n",
    "            assert algorithm_name is not None, experiment_name\n",
    "            train_sigma = find_word(experiment_name, train_sigma_list)\n",
    "            assert train_sigma is not None, experiment_name\n",
    "            results = data[\"results\"]\n",
    "            for dataset_name, reward_list in results.items():\n",
    "                perturbation_summary_path = os.path.join(ROOT_DIR, \"data\", \"xmls\", name, dataset_name, \"perturbation_summary.pkl\")\n",
    "                with open(perturbation_summary_path, \"rb\") as file:\n",
    "                    perturbation_summary = pickle.load(file)\n",
    "                perturbation_names = perturbation_summary[\"perturbations\"]\n",
    "                perturbation_list = perturbation_summary[\"values\"]\n",
    "                for idx, reward in enumerate(reward_list):\n",
    "                    perturbation_dict = dict(zip(perturbation_names, perturbation_list[idx]))\n",
    "                    summary_dict = {\n",
    "                        \"env\": name,\n",
    "                        \"algorithm\": algorithm_name,\n",
    "                        \"train_sigma\": train_sigma,\n",
    "                        \"test_sigma\": dataset_name,\n",
    "                        \"xml_idx\": idx,\n",
    "                        \"reward\": reward\n",
    "                    }\n",
    "                    summary_dict.update(perturbation_dict)\n",
    "                    env_summary_dict[name].append(summary_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df_list = []\n",
    "for env_name, summary_list in env_summary_dict.items():\n",
    "    summary_df = pd.DataFrame(summary_list)\n",
    "    summary_df.replace(\"rma_phase_1\", \"oracle\", inplace=True)\n",
    "    summary_df.replace(\"encoding\", \"oracle\", inplace=True)\n",
    "    summary_df.replace(\"test_sigma_between_0_01\", \"sigma_01\", inplace=True)\n",
    "    summary_df.replace(\"test_sigma_between_0_03\", \"sigma_03\", inplace=True)\n",
    "    summary_df.replace(\"test_sigma_between_0_05\", \"sigma_05\", inplace=True)\n",
    "    summary_df.replace(\"test_sigma_between_0_07\", \"sigma_07\", inplace=True)\n",
    "    summary_df_list.append((env_name, summary_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nans in df ant 0\n",
      "Number of nans in df hopper 0\n",
      "Number of nans in df walker 0\n",
      "Number of nans in df half_cheetah 0\n"
     ]
    }
   ],
   "source": [
    "# Check that there are no nans\n",
    "for summary in summary_df_list:\n",
    "    df = summary[1]\n",
    "    print(\"Number of nans in df\", summary[0], df.isna().any(axis=1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ant ['attention_no_encoding' 'convnet' 'adapted' 'attention' 'simple' 'oracle']\n",
      "hopper ['oracle' 'attention_no_encoding' 'simple' 'adapted' 'convnet' 'attention']\n",
      "walker ['adapted' 'simple' 'oracle' 'attention_no_encoding' 'convnet' 'attention']\n",
      "half_cheetah ['adapted' 'oracle' 'attention' 'simple' 'convnet' 'attention_no_encoding']\n"
     ]
    }
   ],
   "source": [
    "for env_name, df in summary_df_list:\n",
    "    print(env_name, df.algorithm.unique())\n",
    "    for algo in df.algorithm.unique():\n",
    "        for train_sigma in df.train_sigma.unique():\n",
    "            for test_sigma in df.test_sigma.unique():\n",
    "                small_df = df[(df.algorithm == algo) & (df.train_sigma == train_sigma) & (df.test_sigma == test_sigma)]\n",
    "                if len(small_df) != 500:\n",
    "                    print(\"env_name\", env_name, \"algo\", algo, \"train_sigma\", train_sigma, \"test_sigma\", test_sigma, \"number\", len(small_df))\n",
    "    df_name = f\"{env_name}_experiments_df.pkl\"\n",
    "    out_path = os.path.join(ROOT_DIR, \"data\", df_name)\n",
    "    df.to_pickle(out_path)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b7bff28f30f13e0ece4b3d645f4b2dc193f79ccbad83d138c7717a7be8045a4f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('ray16')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
